<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.4" />
<title>aioextensions API documentation</title>
<meta name="description" content="High performance functions to work with the async IO …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>aioextensions</code></h1>
</header>
<section id="section-intro">
<p>High performance functions to work with the async IO.</p>
<p><a href="https://pypi.org/project/aioextensions"><img alt="Release" src="https://img.shields.io/pypi/v/aioextensions?color=success&amp;label=Release&amp;style=flat-square"></a>
<a href="https://kamadorueda.github.io/aioextensions/"><img alt="Documentation" src="https://img.shields.io/badge/Documentation-click_here!-success?style=flat-square"></a>
<a href="https://pypi.org/project/aioextensions"><img alt="Downloads" src="https://img.shields.io/pypi/dm/aioextensions?label=Downloads&amp;style=flat-square"></a>
<a href="https://pypi.org/project/aioextensions"><img alt="Status" src="https://img.shields.io/pypi/status/aioextensions?label=Status&amp;style=flat-square"></a>
<a href="https://kamadorueda.github.io/aioextensions/"><img alt="Coverage" src="https://img.shields.io/badge/Coverage-100%25-success?style=flat-square"></a>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/LICENSE.md"><img alt="License" src="https://img.shields.io/pypi/l/aioextensions?color=success&amp;label=License&amp;style=flat-square"></a></p>
<h1 id="rationale">Rationale</h1>
<p>Modern services deal with a bunch of different tasks to perform:</p>
<p><img alt="Latency comparison" src="https://raw.githubusercontent.com/kamadorueda/aioextensions/latest/docs/static/latency.png"></p>
<p>The important thing to note is that tasks can be categorized in two groups:</p>
<h2 id="cpu-bound-tasks">CPU bound tasks</h2>
<p>Those that happen inside the CPU, with very low latency and exploit the full
potential of the hardware in the computer.</p>
<p><img alt="Resources of an CPU bound task" src="https://raw.githubusercontent.com/kamadorueda/aioextensions/latest/docs/static/resources_cpu_task.png"></p>
<p>Examples of these tasks include:</p>
<table>
<thead>
<tr>
<th>Task</th>
<th align="right">Latency in seconds</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU computation</td>
<td align="right">0.000000001</td>
</tr>
<tr>
<td>Memory access</td>
<td align="right">0.0000001</td>
</tr>
<tr>
<td>CPU Processing (1KB)</td>
<td align="right">0.000003</td>
</tr>
<tr>
<td>Memory read (1MB)</td>
<td align="right">0.00025</td>
</tr>
</tbody>
</table>
<h2 id="io-bound-tasks">IO bound tasks</h2>
<p>Those that happen over a wire that transports data, with very high latencies
and do not exploit the full potential of the hardware because the only thing to
do is waiting until the data gets to the other end and comes back (round-trip).</p>
<p><img alt="Resources of an IO bound task" src="https://raw.githubusercontent.com/kamadorueda/aioextensions/latest/docs/static/resources_io_task.png"></p>
<p>Examples of these tasks include:</p>
<table>
<thead>
<tr>
<th>Task</th>
<th align="right">Latency in seconds</th>
</tr>
</thead>
<tbody>
<tr>
<td>Disk access</td>
<td align="right">0.00015</td>
</tr>
<tr>
<td>HTTP to localhost</td>
<td align="right">0.0005</td>
</tr>
<tr>
<td>Disk read (1MB)</td>
<td align="right">0.02</td>
</tr>
<tr>
<td>HTTP to internet</td>
<td align="right">0.15</td>
</tr>
</tbody>
</table>
<h1 id="speed-and-costs-matter">Speed and costs matter</h1>
<p>At the end of the day, we want to minimize the amount of cost per user served
by the program, server, or service while maximizing the user perception of
speed.</p>
<p>In order to achieve this we need a model that allows us to exploit all CPU
cores and installed hardware in the machine, while maintaining the ability to
query large amounts of high-latency external services over the network:
databases, caches, storage, distributed queues, or input from multiple users.</p>
<h1 id="concurrency-model">Concurrency model</h1>
<p>Python's Async IO has a concurrency model based on an <strong>event loop</strong>, which
is responsible for executing the code, collecting and processing what's needed.</p>
<p>This event-loop executes in the main thread of the Python interpreter and
therefore it's limited by the <a href="https://realpython.com/python-gil">GIL</a>, so it's
alone unable to exploit all hardware installed in the host.</p>
<p>However:</p>
<ul>
<li>CPU intensive work can be sent to a pool of processes, far from the
event-loop and thus being able to bypass the
<a href="https://realpython.com/python-gil">GIL</a>, exploiting many CPU cores in
the machine, and leaving the event-loop schedule and coordinate incoming
requests.</li>
<li>IO intensive work can be sent to a pool of threads, far from the event-loop
and thus being able to wait for high-latency operations without
interrupting the event-loop work.</li>
</ul>
<p>There is an important difference:</p>
<ul>
<li>Work done by a pool of processes is executed in parallel: all CPU cores are
being used.</li>
<li>Work done by a pool of threads is done concurrently: tasks execution is
overlapping, but not necessarily parallel: only 1 task can use the CPU
while the remaining ones are waiting the
<a href="https://realpython.com/python-gil">GIL</a>.</li>
</ul>
<h2 id="solving-cpu-bound-tasks-efficiently">Solving CPU bound tasks efficiently</h2>
<p>The optimal way to perform CPU bound tasks is to send them to separate
processses in order to bypass the <a href="https://realpython.com/python-gil">GIL</a>.</p>
<h2 id="usage">Usage</h2>
<pre><code class="python">&gt;&gt;&gt; from aioextensions import collect, in_process, run
</code></pre>
<pre><code class="python">&gt;&gt;&gt; def cpu_bound_task(id: str):
        print(f'doing: {id}')
        # Imagine here something that uses a lot the CPU
        # For example: this complex mathematical operation
        for _ in range(10): 3**20000000
        print(f'returning: {id}')
        return id
</code></pre>
<pre><code class="python">&gt;&gt;&gt; async def main():
        results = await collect([
            # in_process sends the task to a pool of processes
            in_process(cpu_bound_task, id)
            # Let's solve 5 of those tasks in parallel!
            for id in range(5)
        ])
        print(f'results: {results}')
</code></pre>
<pre><code class="python">&gt;&gt;&gt; run(main())
# I have 4 CPU cores in my machine
doing: 0
doing: 1
doing: 2
doing: 3
returning: 1
doing: 4
returning: 2
returning: 3
returning: 0
returning: 4
results: (0, 1, 2, 3, 4)
</code></pre>
<p>As expected, all CPU cores were used and we were hardware-efficient!</p>
<h2 id="solving-io-bound-tasks-efficiently">Solving IO bound tasks efficiently</h2>
<p>The optimal way to perform IO bound tasks is to send them to separate
threads. This does not bypass the <a href="https://realpython.com/python-gil">GIL</a>.
However, threads will be in idle state most of the time, waiting high-latency
operations to complete.</p>
<h2 id="usage_1">Usage</h2>
<pre><code class="python">&gt;&gt;&gt; from aioextensions import collect, in_thread, run
&gt;&gt;&gt; from time import sleep, time
</code></pre>
<pre><code class="python">&gt;&gt;&gt; def io_bound_task(id: str):
        print(f'time: {time()}, doing: {id}')
        # Imagine here something with high latency
        # For example: a call to the database, or this sleep
        sleep(1)
        print(f'time: {time()}, returning: {id}')
        return id
</code></pre>
<pre><code class="python">&gt;&gt;&gt; async def main():
        results = await collect([
            # in_thread sends the task to a pool of threads
            in_thread(io_bound_task, id)
            # Let's solve 5 of those tasks concurrently!
            for id in range(5)
        ])
        print(f'time: {time()}, results: {results}')
</code></pre>
<pre><code class="python">&gt;&gt;&gt; run(main)
time: 1597623831, doing: 0
time: 1597623831, doing: 1
time: 1597623831, doing: 2
time: 1597623831, doing: 3
time: 1597623831, doing: 4
time: 1597623832, returning: 0
time: 1597623832, returning: 4
time: 1597623832, returning: 3
time: 1597623832, returning: 2
time: 1597623832, returning: 1
time: 1597623832, results: (0, 1, 2, 3, 4)
</code></pre>
<p>As expected, all tasks were executed concurrently. This means that instead of
waiting five seconds for five tasks (serially) we just waited one second for
all of them.</p>
<h1 id="installing">Installing</h1>
<pre><code>$ pip install aioextensions
</code></pre>
<h1 id="using">Using</h1>
<pre><code>&gt;&gt;&gt; from aioextensions import *  # to import everything
</code></pre>
<p>Please read the documentation bellow for more details about every function.</p>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/src/aioextensions/__init__.py#L0-L793" class="git-link">Browse git</a>
</summary>
<pre><code class="python">&#34;&#34;&#34;High performance functions to work with the async IO.

[![Release](
https://img.shields.io/pypi/v/aioextensions?color=success&amp;label=Release&amp;style=flat-square)](
https://pypi.org/project/aioextensions)
[![Documentation](
https://img.shields.io/badge/Documentation-click_here!-success?style=flat-square)](
https://kamadorueda.github.io/aioextensions/)
[![Downloads](
https://img.shields.io/pypi/dm/aioextensions?label=Downloads&amp;style=flat-square)](
https://pypi.org/project/aioextensions)
[![Status](
https://img.shields.io/pypi/status/aioextensions?label=Status&amp;style=flat-square)](
https://pypi.org/project/aioextensions)
[![Coverage](
https://img.shields.io/badge/Coverage-100%25-success?style=flat-square)](
https://kamadorueda.github.io/aioextensions/)
[![License](
https://img.shields.io/pypi/l/aioextensions?color=success&amp;label=License&amp;style=flat-square)](
https://github.com/kamadorueda/aioextensions/blob/latest/LICENSE.md)

# Rationale

Modern services deal with a bunch of different tasks to perform:

![Latency comparison](
https://raw.githubusercontent.com/kamadorueda/aioextensions/latest/docs/static/latency.png)

The important thing to note is that tasks can be categorized in two groups:

## CPU bound tasks

Those that happen inside the CPU, with very low latency and exploit the full
potential of the hardware in the computer.

![Resources of an CPU bound task](
https://raw.githubusercontent.com/kamadorueda/aioextensions/latest/docs/static/resources_cpu_task.png)

Examples of these tasks include:

| Task                 | Latency in seconds |
|----------------------|-------------------:|
| CPU computation      |        0.000000001 |
| Memory access        |          0.0000001 |
| CPU Processing (1KB) |           0.000003 |
| Memory read (1MB)    |            0.00025 |

## IO bound tasks

Those that happen over a wire that transports data, with very high latencies
and do not exploit the full potential of the hardware because the only thing to
do is waiting until the data gets to the other end and comes back (round-trip).

![Resources of an IO bound task](
https://raw.githubusercontent.com/kamadorueda/aioextensions/latest/docs/static/resources_io_task.png)

Examples of these tasks include:

| Task                 | Latency in seconds |
|----------------------|-------------------:|
| Disk access          |            0.00015 |
| HTTP to localhost    |             0.0005 |
| Disk read (1MB)      |               0.02 |
| HTTP to internet     |               0.15 |

# Speed and costs matter

At the end of the day, we want to minimize the amount of cost per user served
by the program, server, or service while maximizing the user perception of
speed.

In order to achieve this we need a model that allows us to exploit all CPU
cores and installed hardware in the machine, while maintaining the ability to
query large amounts of high-latency external services over the network:
databases, caches, storage, distributed queues, or input from multiple users.

# Concurrency model

Python&#39;s Async IO has a concurrency model based on an **event loop**, which
is responsible for executing the code, collecting and processing what&#39;s needed.

This event-loop executes in the main thread of the Python interpreter and
therefore it&#39;s limited by the [GIL](https://realpython.com/python-gil), so it&#39;s
alone unable to exploit all hardware installed in the host.

However:

- CPU intensive work can be sent to a pool of processes, far from the
    event-loop and thus being able to bypass the
    [GIL](https://realpython.com/python-gil), exploiting many CPU cores in
    the machine, and leaving the event-loop schedule and coordinate incoming
    requests.
- IO intensive work can be sent to a pool of threads, far from the event-loop
    and thus being able to wait for high-latency operations without
    interrupting the event-loop work.

There is an important difference:

-  Work done by a pool of processes is executed in parallel: all CPU cores are
    being used.
-  Work done by a pool of threads is done concurrently: tasks execution is
    overlapping, but not necessarily parallel: only 1 task can use the CPU
    while the remaining ones are waiting the
    [GIL](https://realpython.com/python-gil).

## Solving CPU bound tasks efficiently

The optimal way to perform CPU bound tasks is to send them to separate
processses in order to bypass the [GIL](https://realpython.com/python-gil).

Usage:

    &gt;&gt;&gt; from aioextensions import collect, in_process, run

    &gt;&gt;&gt; def cpu_bound_task(id: str):
            print(f&#39;doing: {id}&#39;)
            # Imagine here something that uses a lot the CPU
            # For example: this complex mathematical operation
            for _ in range(10): 3**20000000
            print(f&#39;returning: {id}&#39;)
            return id

    &gt;&gt;&gt; async def main():
            results = await collect([
                # in_process sends the task to a pool of processes
                in_process(cpu_bound_task, id)
                # Let&#39;s solve 5 of those tasks in parallel!
                for id in range(5)
            ])
            print(f&#39;results: {results}&#39;)

    &gt;&gt;&gt; run(main())
    # I have 4 CPU cores in my machine
    doing: 0
    doing: 1
    doing: 2
    doing: 3
    returning: 1
    doing: 4
    returning: 2
    returning: 3
    returning: 0
    returning: 4
    results: (0, 1, 2, 3, 4)

As expected, all CPU cores were used and we were hardware-efficient!

## Solving IO bound tasks efficiently

The optimal way to perform IO bound tasks is to send them to separate
threads. This does not bypass the [GIL](https://realpython.com/python-gil).
However, threads will be in idle state most of the time, waiting high-latency
operations to complete.

Usage:

    &gt;&gt;&gt; from aioextensions import collect, in_thread, run
    &gt;&gt;&gt; from time import sleep, time

    &gt;&gt;&gt; def io_bound_task(id: str):
            print(f&#39;time: {time()}, doing: {id}&#39;)
            # Imagine here something with high latency
            # For example: a call to the database, or this sleep
            sleep(1)
            print(f&#39;time: {time()}, returning: {id}&#39;)
            return id

    &gt;&gt;&gt; async def main():
            results = await collect([
                # in_thread sends the task to a pool of threads
                in_thread(io_bound_task, id)
                # Let&#39;s solve 5 of those tasks concurrently!
                for id in range(5)
            ])
            print(f&#39;time: {time()}, results: {results}&#39;)

    &gt;&gt;&gt; run(main)
    time: 1597623831, doing: 0
    time: 1597623831, doing: 1
    time: 1597623831, doing: 2
    time: 1597623831, doing: 3
    time: 1597623831, doing: 4
    time: 1597623832, returning: 0
    time: 1597623832, returning: 4
    time: 1597623832, returning: 3
    time: 1597623832, returning: 2
    time: 1597623832, returning: 1
    time: 1597623832, results: (0, 1, 2, 3, 4)

As expected, all tasks were executed concurrently. This means that instead of
waiting five seconds for five tasks (serially) we just waited one second for
all of them.

# Installing

    $ pip install aioextensions

# Using

    &gt;&gt;&gt; from aioextensions import *  # to import everything

Please read the documentation bellow for more details about every function.
&#34;&#34;&#34;

# Standard library
import asyncio
from collections import (
    deque,
)
from concurrent.futures import (
    Executor,
    ProcessPoolExecutor,
    ThreadPoolExecutor,
)
from contextlib import suppress
from functools import (
    partial,
    wraps,
)
from itertools import (
    tee,
)
from os import (
    cpu_count,
)
from typing import (
    Any,
    AsyncGenerator,
    Awaitable,
    Callable,
    cast,
    Deque,
    Dict,
    Generator,
    Iterable,
    Optional,
    Tuple,
    Type,
    TypeVar,
    Union,
)

# Third party libraries
import uvloop

# Constants
F = TypeVar(&#39;F&#39;, bound=Callable[..., Any])  # pylint: disable=invalid-name
S = TypeVar(&#39;S&#39;)  # pylint: disable=invalid-name
T = TypeVar(&#39;T&#39;)  # pylint: disable=invalid-name
Y = TypeVar(&#39;Y&#39;)  # pylint: disable=invalid-name

# Linters
# pylint: disable=unsubscriptable-object


def run(coroutine: Awaitable[T], *, debug: bool = False) -&gt; T:
    &#34;&#34;&#34;Execute an asynchronous function synchronously and return its result.

    Example:

        &gt;&gt;&gt; async def do(a, b=0):
                await something
                return a + b

        &gt;&gt;&gt; run(do(1, b=2))

        &gt;&gt;&gt; 3

    This function acts as a drop-in replacement of asyncio.run and
    installs `uvloop` (the fastest event-loop implementation out there) first.

    .. tip::
        Use this as the entrypoint for your program.
    &#34;&#34;&#34;
    uvloop.install()
    return asyncio.run(coroutine, debug=debug)


async def in_thread(
    function: Callable[..., T],
    *args: Any,
    **kwargs: Any,
) -&gt; T:
    &#34;&#34;&#34;Execute `function(*args, **kwargs)` in the configured thread pool.

    This is the most performant wrapper for IO bound and high-latency tasks.

    Every task will be assigned at most one thread, if there are more tasks
    than threads in the pool the excess will be executed in FIFO order.

    Spawning a million IO bound tasks with this function has a very small
    memory footprint.

    .. warning::
        Executing CPU intensive work here is a bad idea because of the
        limitations that the [GIL](https://realpython.com/python-gil) imposes.

        See `in_process` for a CPU performant alternative.
    &#34;&#34;&#34;
    _ensure_thread_pool_is_initialized()

    return await asyncio.get_running_loop().run_in_executor(
        THREAD_POOL.pool, partial(function, *args, **kwargs),
    )


async def in_process(
    function: Callable[..., T],
    *args: Any,
    **kwargs: Any,
) -&gt; T:
    &#34;&#34;&#34;Execute `function(*args, **kwargs)` in the configured process pool.

    This is the most performant wrapper for CPU bound and low-latency tasks.

    Tasks executed in a process pool bypass the
    [GIL](https://realpython.com/python-gil) and can consume all CPU cores
    available in the host if needed.

    Every task will be assigned at most one process, if there are more tasks
    than processes in the pool the excess will be executed in FIFO order.

    .. warning::
        Executing IO intensive work here is possible, but spawning a process
        has some overhead that can be avoided using threads at no performance
        expense.

        See `in_thread` for an IO performant alternative.
    &#34;&#34;&#34;
    _ensure_process_pool_is_initialized()

    return await asyncio.get_running_loop().run_in_executor(
        PROCESS_POOL.pool, partial(function, *args, **kwargs),
    )


def rate_limited(
    *,
    max_calls: int,
    over_seconds: Union[float, int],
) -&gt; Callable[[F], F]:
    &#34;&#34;&#34;Decorator to turn an asynchronous function into a rate limited one.

    The decorated function won&#39;t be able to execute more than `max_calls` times
    over a period of `over_seconds` seconds. The excess will be queued in FIFO
    mode.

    Example:

        If you want to perform at most 2 calls to a database per second:

        &gt;&gt;&gt; @rate_limited(max_calls=2, over_seconds=1.0)
            async def query(n):
                await something
                print(f&#39;time: {time()}, doing: {n}&#39;)

        &gt;&gt;&gt; for n in range(10):
                await query(n)

    Output:

        ```
        time: 1597701092, doing: 0
        time: 1597701092, doing: 1
        time: 1597701093, doing: 2
        time: 1597701093, doing: 3
        time: 1597701094, doing: 4
        time: 1597701094, doing: 5
        time: 1597701095, doing: 6
        time: 1597701095, doing: 7
        time: 1597701096, doing: 8
        time: 1597701096, doing: 9
        ```

        Take into account that calls can still burst.
    &#34;&#34;&#34;

    def decorator(function: F) -&gt; F:
        lock = None
        waits: Deque[float] = deque()

        @wraps(function)
        async def wrapper(*args: Any, **kwargs: Any) -&gt; Any:
            nonlocal lock

            lock = lock or asyncio.Lock()
            loop = asyncio.get_event_loop()

            async with lock:
                while len(waits) &gt;= max_calls:
                    await asyncio.sleep(waits.popleft() - loop.time())
                waits.append(loop.time() + over_seconds)

            return await function(*args, **kwargs)

        return cast(F, wrapper)

    return decorator


async def collect(
    awaitables: Iterable[Awaitable[T]],
    *,
    workers: int = 1024,
    worker_greediness: int = 0,
) -&gt; Tuple[T, ...]:
    &#34;&#34;&#34;Resolve concurrently the input stream and return back in the same order.

    The algorithm makes sure that at any point in time every worker is busy.
    Also, at any point in time there will be at most _number of `workers`_
    tasks being resolved concurrently.

    The `worker_greediness` parameter controlls how much each worker can
    process before waiting for you to retrieve results. This is important when
    the input stream is big as it allows you to control memory usage.

    Usage:
        &gt;&gt;&gt; async def do(n):
                print(f&#39;running: {n}&#39;)
                await sleep(1)
                print(f&#39;returning: {n}&#39;)
                return n

        &gt;&gt;&gt; iterable = map(do, range(5))

        &gt;&gt;&gt; results = await collect(iterable, workers=2)

        &gt;&gt;&gt; print(f&#39;results: {results}&#39;)

    Output:
        ```
        running: 0
        running: 1
        returning: 0
        returning: 1
        running: 2
        running: 3
        returning: 2
        returning: 3
        running: 4
        returning: 4
        results: (0, 1, 2, 3, 4)
        ```

    .. tip::
        This is similar to asyncio.as_completed. However this returns results
        in order and allows you to control how much resources are consumed
        throughout the execution, for instance:

        - How many open files will be opened at the same time
        - How many HTTP requests will be performed to a service (rate limit)
        - How many sockets will be opened concurrently
        - Etc

        This is useful for finite resources, for instance: the number
        of sockets provided by the operative system is limited; going beyond it
        would make the kernel to kill the program abruptly.

    Args:
        awaitables: An iterable (generator, list, tuple, set, etc) of
            awaitables (coroutine, asyncio.Task, or asyncio.Future).
        workers: The number of independent workers that will be processing
            the input stream.
        worker_greediness: How much tasks can a worker process before waiting
            for you to retrieve its results. 0 means unlimited.

    Yields:
        A future with the result of the next ready task. Futures are yielded in
        the same order of the input stream (opposite to asyncio.as_completed)

    .. tip::
        This approach may be many times faster than batching because
        workers are independent of each other and they are constantly fetching
        the next task as soon as they are free (if greediness allows them).

    .. tip::
        If awaitables is an instance of Sized (has `__len__` prototype).
        This function will launch at most `len(awaitables)` workers.
    &#34;&#34;&#34;
    return tuple([
        await elem
        for elem in resolve(
            awaitables,
            workers=workers,
            worker_greediness=worker_greediness,
        )
    ])


def resolve(  # noqa: mccabe
    awaitables: Iterable[Awaitable[T]],
    *,
    workers: int = 1024,
    worker_greediness: int = 0,
) -&gt; Iterable[Awaitable[T]]:
    &#34;&#34;&#34;Resolve concurrently the input stream and yield back in the same order.

    This works very similar to `collect` except it&#39;s lazy and allows you
    to handle exceptions on singular elements.

    Usage:
        &gt;&gt;&gt; async def do(n):
                print(f&#39;running: {n}&#39;)
                await asyncio.sleep(1)
                print(f&#39;returning: {n}&#39;)
                return n

        &gt;&gt;&gt; iterable = map(do, range(5))

        &gt;&gt;&gt; for next in resolve(iterable, workers=2):
                try:
                    print(f&#39;got resolved result: {await next}&#39;)
                except:
                    pass  # Handle possible exceptions

    Output:
        ```
        running: 0
        running: 1
        returning: 0
        returning: 1
        got resolved result: 0
        got resolved result: 1
        running: 2
        running: 3
        returning: 2
        returning: 3
        got resolved result: 2
        got resolved result: 3
        running: 4
        returning: 4
        got resolved result: 4
        ```

    See `collect` for more information on the algorithm used and parameters.
    &#34;&#34;&#34;
    if workers &lt; 1:
        raise ValueError(&#39;workers must be &gt;= 1&#39;)
    if worker_greediness &lt; 0:
        raise ValueError(&#39;worker_greediness must be &gt;= 0&#39;)

    if hasattr(awaitables, &#39;__len__&#39;):
        workers = min(workers, len(awaitables))  # type: ignore

    loop = asyncio.get_event_loop()
    store: Dict[int, asyncio.Queue] = {}
    stream, stream_copy = tee(enumerate(awaitables))
    stream_finished = asyncio.Event()
    workers_up = asyncio.Event()
    workers_tasks: Dict[int, asyncio.Task] = {}

    async def worker() -&gt; None:
        done: asyncio.Queue = asyncio.Queue(worker_greediness)
        for index, awaitable in stream:
            store[index] = done
            future = loop.create_future()
            future.set_result(await schedule(awaitable, loop=loop))
            await done.put(future)
            workers_up.set()
        workers_up.set()
        stream_finished.set()

    async def start_workers() -&gt; None:
        for index in range(workers):
            if stream_finished.is_set():
                break
            workers_tasks[index] = asyncio.create_task(worker())
            await force_loop_cycle()
        await workers_up.wait()

    async def get_one(index: int) -&gt; Awaitable[T]:
        if not workers_tasks:
            await start_workers()

        awaitable = await store.pop(index).get()
        result: Awaitable[T] = (await awaitable).result()
        return result

    for index, _ in stream_copy:
        yield cast(Awaitable[T], get_one(index))


async def force_loop_cycle() -&gt; None:
    &#34;&#34;&#34;Force the event loop to perform one cycle.

    This can be used to suspend the execution of the current coroutine and
    yield control back to the event-loop until the next cycle.

    Can be seen as a forceful switch of control between threads.
    Useful for cooperative initialization.

    Usage:

        &gt;&gt;&gt; await forceforce_loop_cycle()

    &#34;&#34;&#34;
    await asyncio.sleep(0)


async def generate_in_thread(
    generator_func: Callable[..., Generator[Y, S, None]],
    *args: Any,
    **kwargs: Any,
) -&gt; AsyncGenerator[Y, S]:
    &#34;&#34;&#34;Mimic `generator_func(*args, **kwargs)` in the configured thread pool.

    Note that `generator_func(*args, **kwargs)` may return a generator or an
    interator and both cases are handled rightfully.

    Usage:

        &gt;&gt;&gt; from os import scandir

        &gt;&gt;&gt; async for entry in generate_in_thread(scandir, &#39;.&#39;):
                print(entry.name)

    Output:
        ```
        .gitignore
        LICENSE.md
        README.md
        ...
        ```

    Calls to the generator are done serially and not concurrently.

    The benefit of wrapping a generator with this function is that the
    event-loop is free to schedule and wait another tasks in the mean time.
    For instance, in a web server.
    &#34;&#34;&#34;
    gen: Generator[Y, S, None] = generator_func(*args, **kwargs)
    gen_sent: Any = None

    def gen_next(val: S) -&gt; Y:
        with suppress(StopIteration):
            return gen.send(val) if hasattr(gen, &#39;send&#39;) else next(gen)
        raise StopAsyncIteration()

    while True:
        try:
            gen_sent = yield await in_thread(gen_next, gen_sent)
        except StopAsyncIteration:
            return


def schedule(
    awaitable: Awaitable[T],
    *,
    loop: Optional[asyncio.AbstractEventLoop] = None,
) -&gt; &#39;Awaitable[asyncio.Future[T]]&#39;:
    &#34;&#34;&#34;Schedule an awaitable in the event loop and return a wrapper for it.

    Usage:

        &gt;&gt;&gt; async def do(n):
                print(f&#39;running: {n}&#39;)
                await sleep(1)
                print(f&#39;returning: {n}&#39;)

        &gt;&gt;&gt; task = schedule(do(3))  # Task is executing in the background now

        &gt;&gt;&gt; print(&#39;other work is being done here&#39;)

        &gt;&gt;&gt; task_result = await task  # Wait until the task is ready

        &gt;&gt;&gt; print(f&#39;result: {task_result.result()}&#39;)  # may rise if do() raised

    Output:

        ```
        other work is being done here
        doing: 3
        returning: 3
        3
        ```

    This works very similar to asyncio.create_task. The main difference is that
    the result (or exception) can be accessed via exception() or result()
    methods.

    If an exception was raised by the awaitable, it will be propagated only at
    the moment result() is called and never otherwise.
    &#34;&#34;&#34;
    wrapper = (loop or asyncio.get_event_loop()).create_future()

    def _done_callback(future: asyncio.Future) -&gt; None:
        if not wrapper.done():
            wrapper.set_result(future)

    asyncio.create_task(awaitable).add_done_callback(_done_callback)

    return wrapper


def _ensure_process_pool_is_initialized() -&gt; None:
    if not PROCESS_POOL.initialized:
        PROCESS_POOL.initialize(max_workers=CPU_CORES)


def _ensure_thread_pool_is_initialized() -&gt; None:
    if not THREAD_POOL.initialized:
        THREAD_POOL.initialize(max_workers=10 * CPU_CORES)


class ExecutorPool:
    &#34;&#34;&#34;Object representing a pool of Processes or Threads.

    The actual pool is created at `initialization` time
    and it is empty until that.
    &#34;&#34;&#34;
    def __init__(
        self,
        cls: Union[
            Type[ProcessPoolExecutor],
            Type[ThreadPoolExecutor],
        ],
    ) -&gt; None:
        self._cls = cls
        self._pool: Optional[Executor] = None

    def initialize(self, *, max_workers: Optional[int] = None) -&gt; None:
        &#34;&#34;&#34;Initialize the executor with a cap of at most `max_workers`.

        Workers are created on-demand as needed or never created at all
        if never needed.
        &#34;&#34;&#34;
        if self._pool is not None:
            self._pool.shutdown(wait=False)

        self._pool = self._cls(max_workers=max_workers)

    def shutdown(self, *, wait: bool) -&gt; None:
        &#34;&#34;&#34;Shut down the executor and (optionally) waits for workers to finish.
        &#34;&#34;&#34;
        if self._pool is not None:
            self._pool.shutdown(wait=wait)
            self._pool = None

    @property
    def pool(self) -&gt; Executor:
        &#34;&#34;&#34;Low level pool of workers held by the executor, may be None.&#34;&#34;&#34;
        if self._pool is None:
            raise RuntimeError(&#39;Must call initialize first&#39;)

        return self._pool

    @property
    def initialized(self) -&gt; bool:
        &#34;&#34;&#34;Return true if the executor is initialized and ready to process.&#34;&#34;&#34;
        return self._pool is not None


def run_decorator(function: F) -&gt; F:
    &#34;&#34;&#34;Decorator to turn an asynchronous function into a synchronous one.

    Example:
        &gt;&gt;&gt; @run_decorator
            async def do(a, b=0):
                return a + b

        &gt;&gt;&gt; do(1, b=2) == 3

    This can be used as a bridge between synchronous and asynchronous code.
    We use it mostly in tests for its convenience over pytest-asyncio plugin.
    &#34;&#34;&#34;

    @wraps(function)
    def wrapper(*args: Any, **kwargs: Any) -&gt; Any:
        return run(function(*args, **kwargs))

    return cast(F, wrapper)


# Constants
CPU_CORES: int = cpu_count() or 1
&#34;&#34;&#34;Number of CPU cores in the host system.&#34;&#34;&#34;

PROCESS_POOL: ExecutorPool = ExecutorPool(ProcessPoolExecutor)
&#34;&#34;&#34;Process pool used by `in_process` function to execute work.

Preconfigured to launch at most `CPU_CORES` processes (if needed).

Proceses are created on the first `in_process` call, one by one as needed
or never launched otherwise.
&#34;&#34;&#34;

THREAD_POOL: ExecutorPool = ExecutorPool(ThreadPoolExecutor)
&#34;&#34;&#34;Thread pool used by `in_thread` function to execute work.

Preconfigured to launch at most 10 * `CPU_CORES` threads (if needed).

Threads are created on the first `in_thread` call, one by one as needed,
or never launched otherwise.
&#34;&#34;&#34;</code></pre>
</details>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="aioextensions.CPU_CORES"><code class="name">var <span class="ident">CPU_CORES</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of CPU cores in the host system.</p></div>
</dd>
<dt id="aioextensions.PROCESS_POOL"><code class="name">var <span class="ident">PROCESS_POOL</span> : <a title="aioextensions.ExecutorPool" href="#aioextensions.ExecutorPool">ExecutorPool</a></code></dt>
<dd>
<div class="desc"><p>Process pool used by <code><a title="aioextensions.in_process" href="#aioextensions.in_process">in_process()</a></code> function to execute work.</p>
<p>Preconfigured to launch at most <code><a title="aioextensions.CPU_CORES" href="#aioextensions.CPU_CORES">CPU_CORES</a></code> processes (if needed).</p>
<p>Proceses are created on the first <code><a title="aioextensions.in_process" href="#aioextensions.in_process">in_process()</a></code> call, one by one as needed
or never launched otherwise.</p></div>
</dd>
<dt id="aioextensions.THREAD_POOL"><code class="name">var <span class="ident">THREAD_POOL</span> : <a title="aioextensions.ExecutorPool" href="#aioextensions.ExecutorPool">ExecutorPool</a></code></dt>
<dd>
<div class="desc"><p>Thread pool used by <code><a title="aioextensions.in_thread" href="#aioextensions.in_thread">in_thread()</a></code> function to execute work.</p>
<p>Preconfigured to launch at most 10 * <code><a title="aioextensions.CPU_CORES" href="#aioextensions.CPU_CORES">CPU_CORES</a></code> threads (if needed).</p>
<p>Threads are created on the first <code><a title="aioextensions.in_thread" href="#aioextensions.in_thread">in_thread()</a></code> call, one by one as needed,
or never launched otherwise.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="aioextensions.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>coroutine: Awaitable[~T], *, debug: bool = False) ‑> ~T</span>
</code></dt>
<dd>
<div class="desc"><p>Execute an asynchronous function synchronously and return its result.</p>
<h2 id="example">Example</h2>
<pre><code class="python">&gt;&gt;&gt; async def do(a, b=0):
        await something
        return a + b
</code></pre>
<pre><code class="python">&gt;&gt;&gt; run(do(1, b=2))
</code></pre>
<pre><code class="python">&gt;&gt;&gt; 3
</code></pre>
<p>This function acts as a drop-in replacement of asyncio.run and
installs <code>uvloop</code> (the fastest event-loop implementation out there) first.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Use this as the entrypoint for your program.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/src/aioextensions/__init__.py#L256-L276" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def run(coroutine: Awaitable[T], *, debug: bool = False) -&gt; T:
    &#34;&#34;&#34;Execute an asynchronous function synchronously and return its result.

    Example:

        &gt;&gt;&gt; async def do(a, b=0):
                await something
                return a + b

        &gt;&gt;&gt; run(do(1, b=2))

        &gt;&gt;&gt; 3

    This function acts as a drop-in replacement of asyncio.run and
    installs `uvloop` (the fastest event-loop implementation out there) first.

    .. tip::
        Use this as the entrypoint for your program.
    &#34;&#34;&#34;
    uvloop.install()
    return asyncio.run(coroutine, debug=debug)</code></pre>
</details>
</dd>
<dt id="aioextensions.in_thread"><code class="name flex">
<span>async def <span class="ident">in_thread</span></span>(<span>function: Callable[..., ~T], *args: Any, **kwargs: Any) ‑> ~T</span>
</code></dt>
<dd>
<div class="desc"><p>Execute <code>function(*args, **kwargs)</code> in the configured thread pool.</p>
<p>This is the most performant wrapper for IO bound and high-latency tasks.</p>
<p>Every task will be assigned at most one thread, if there are more tasks
than threads in the pool the excess will be executed in FIFO order.</p>
<p>Spawning a million IO bound tasks with this function has a very small
memory footprint.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Executing CPU intensive work here is a bad idea because of the
limitations that the <a href="https://realpython.com/python-gil">GIL</a> imposes.</p>
<p>See <code><a title="aioextensions.in_process" href="#aioextensions.in_process">in_process()</a></code> for a CPU performant alternative.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/src/aioextensions/__init__.py#L279-L304" class="git-link">Browse git</a>
</summary>
<pre><code class="python">async def in_thread(
    function: Callable[..., T],
    *args: Any,
    **kwargs: Any,
) -&gt; T:
    &#34;&#34;&#34;Execute `function(*args, **kwargs)` in the configured thread pool.

    This is the most performant wrapper for IO bound and high-latency tasks.

    Every task will be assigned at most one thread, if there are more tasks
    than threads in the pool the excess will be executed in FIFO order.

    Spawning a million IO bound tasks with this function has a very small
    memory footprint.

    .. warning::
        Executing CPU intensive work here is a bad idea because of the
        limitations that the [GIL](https://realpython.com/python-gil) imposes.

        See `in_process` for a CPU performant alternative.
    &#34;&#34;&#34;
    _ensure_thread_pool_is_initialized()

    return await asyncio.get_running_loop().run_in_executor(
        THREAD_POOL.pool, partial(function, *args, **kwargs),
    )</code></pre>
</details>
</dd>
<dt id="aioextensions.in_process"><code class="name flex">
<span>async def <span class="ident">in_process</span></span>(<span>function: Callable[..., ~T], *args: Any, **kwargs: Any) ‑> ~T</span>
</code></dt>
<dd>
<div class="desc"><p>Execute <code>function(*args, **kwargs)</code> in the configured process pool.</p>
<p>This is the most performant wrapper for CPU bound and low-latency tasks.</p>
<p>Tasks executed in a process pool bypass the
<a href="https://realpython.com/python-gil">GIL</a> and can consume all CPU cores
available in the host if needed.</p>
<p>Every task will be assigned at most one process, if there are more tasks
than processes in the pool the excess will be executed in FIFO order.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Executing IO intensive work here is possible, but spawning a process
has some overhead that can be avoided using threads at no performance
expense.</p>
<p>See <code><a title="aioextensions.in_thread" href="#aioextensions.in_thread">in_thread()</a></code> for an IO performant alternative.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/src/aioextensions/__init__.py#L307-L334" class="git-link">Browse git</a>
</summary>
<pre><code class="python">async def in_process(
    function: Callable[..., T],
    *args: Any,
    **kwargs: Any,
) -&gt; T:
    &#34;&#34;&#34;Execute `function(*args, **kwargs)` in the configured process pool.

    This is the most performant wrapper for CPU bound and low-latency tasks.

    Tasks executed in a process pool bypass the
    [GIL](https://realpython.com/python-gil) and can consume all CPU cores
    available in the host if needed.

    Every task will be assigned at most one process, if there are more tasks
    than processes in the pool the excess will be executed in FIFO order.

    .. warning::
        Executing IO intensive work here is possible, but spawning a process
        has some overhead that can be avoided using threads at no performance
        expense.

        See `in_thread` for an IO performant alternative.
    &#34;&#34;&#34;
    _ensure_process_pool_is_initialized()

    return await asyncio.get_running_loop().run_in_executor(
        PROCESS_POOL.pool, partial(function, *args, **kwargs),
    )</code></pre>
</details>
</dd>
<dt id="aioextensions.rate_limited"><code class="name flex">
<span>def <span class="ident">rate_limited</span></span>(<span>*, max_calls: int, over_seconds: Union[float, int]) ‑> Callable[[~F], ~F]</span>
</code></dt>
<dd>
<div class="desc"><p>Decorator to turn an asynchronous function into a rate limited one.</p>
<p>The decorated function won't be able to execute more than <code>max_calls</code> times
over a period of <code>over_seconds</code> seconds. The excess will be queued in FIFO
mode.</p>
<h2 id="example">Example</h2>
<p>If you want to perform at most 2 calls to a database per second:</p>
<pre><code class="python">&gt;&gt;&gt; @rate_limited(max_calls=2, over_seconds=1.0)
    async def query(n):
        await something
        print(f'time: {time()}, doing: {n}')
</code></pre>
<pre><code class="python">&gt;&gt;&gt; for n in range(10):
        await query(n)
</code></pre>
<h2 id="output">Output</h2>
<pre><code>time: 1597701092, doing: 0
time: 1597701092, doing: 1
time: 1597701093, doing: 2
time: 1597701093, doing: 3
time: 1597701094, doing: 4
time: 1597701094, doing: 5
time: 1597701095, doing: 6
time: 1597701095, doing: 7
time: 1597701096, doing: 8
time: 1597701096, doing: 9
</code></pre>
<p>Take into account that calls can still burst.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/src/aioextensions/__init__.py#L337-L398" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def rate_limited(
    *,
    max_calls: int,
    over_seconds: Union[float, int],
) -&gt; Callable[[F], F]:
    &#34;&#34;&#34;Decorator to turn an asynchronous function into a rate limited one.

    The decorated function won&#39;t be able to execute more than `max_calls` times
    over a period of `over_seconds` seconds. The excess will be queued in FIFO
    mode.

    Example:

        If you want to perform at most 2 calls to a database per second:

        &gt;&gt;&gt; @rate_limited(max_calls=2, over_seconds=1.0)
            async def query(n):
                await something
                print(f&#39;time: {time()}, doing: {n}&#39;)

        &gt;&gt;&gt; for n in range(10):
                await query(n)

    Output:

        ```
        time: 1597701092, doing: 0
        time: 1597701092, doing: 1
        time: 1597701093, doing: 2
        time: 1597701093, doing: 3
        time: 1597701094, doing: 4
        time: 1597701094, doing: 5
        time: 1597701095, doing: 6
        time: 1597701095, doing: 7
        time: 1597701096, doing: 8
        time: 1597701096, doing: 9
        ```

        Take into account that calls can still burst.
    &#34;&#34;&#34;

    def decorator(function: F) -&gt; F:
        lock = None
        waits: Deque[float] = deque()

        @wraps(function)
        async def wrapper(*args: Any, **kwargs: Any) -&gt; Any:
            nonlocal lock

            lock = lock or asyncio.Lock()
            loop = asyncio.get_event_loop()

            async with lock:
                while len(waits) &gt;= max_calls:
                    await asyncio.sleep(waits.popleft() - loop.time())
                waits.append(loop.time() + over_seconds)

            return await function(*args, **kwargs)

        return cast(F, wrapper)

    return decorator</code></pre>
</details>
</dd>
<dt id="aioextensions.collect"><code class="name flex">
<span>async def <span class="ident">collect</span></span>(<span>awaitables: Iterable[Awaitable[~T]], *, workers: int = 1024, worker_greediness: int = 0) ‑> Tuple[~T, ...]</span>
</code></dt>
<dd>
<div class="desc"><p>Resolve concurrently the input stream and return back in the same order.</p>
<p>The algorithm makes sure that at any point in time every worker is busy.
Also, at any point in time there will be at most <em>number of <code>workers</code></em>
tasks being resolved concurrently.</p>
<p>The <code>worker_greediness</code> parameter controlls how much each worker can
process before waiting for you to retrieve results. This is important when
the input stream is big as it allows you to control memory usage.</p>
<h2 id="usage">Usage</h2>
<pre><code class="python">&gt;&gt;&gt; async def do(n):
        print(f'running: {n}')
        await sleep(1)
        print(f'returning: {n}')
        return n
</code></pre>
<pre><code class="python">&gt;&gt;&gt; iterable = map(do, range(5))
</code></pre>
<pre><code class="python">&gt;&gt;&gt; results = await collect(iterable, workers=2)
</code></pre>
<pre><code class="python">&gt;&gt;&gt; print(f'results: {results}')
</code></pre>
<h2 id="output">Output</h2>
<pre><code>running: 0
running: 1
returning: 0
returning: 1
running: 2
running: 3
returning: 2
returning: 3
running: 4
returning: 4
results: (0, 1, 2, 3, 4)
</code></pre>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>This is similar to asyncio.as_completed. However this returns results
in order and allows you to control how much resources are consumed
throughout the execution, for instance:</p>
<ul>
<li>How many open files will be opened at the same time</li>
<li>How many HTTP requests will be performed to a service (rate limit)</li>
<li>How many sockets will be opened concurrently</li>
<li>Etc</li>
</ul>
<p>This is useful for finite resources, for instance: the number
of sockets provided by the operative system is limited; going beyond it
would make the kernel to kill the program abruptly.</p>
</div>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>awaitables</code></strong></dt>
<dd>An iterable (generator, list, tuple, set, etc) of
awaitables (coroutine, asyncio.Task, or asyncio.Future).</dd>
<dt><strong><code>workers</code></strong></dt>
<dd>The number of independent workers that will be processing
the input stream.</dd>
<dt><strong><code>worker_greediness</code></strong></dt>
<dd>How much tasks can a worker process before waiting
for you to retrieve its results. 0 means unlimited.</dd>
</dl>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>This approach may be many times faster than batching because
workers are independent of each other and they are constantly fetching
the next task as soon as they are free (if greediness allows them).</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If awaitables is an instance of Sized (has <code>__len__</code> prototype).
This function will launch at most <code>len(awaitables)</code> workers.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/src/aioextensions/__init__.py#L401-L487" class="git-link">Browse git</a>
</summary>
<pre><code class="python">async def collect(
    awaitables: Iterable[Awaitable[T]],
    *,
    workers: int = 1024,
    worker_greediness: int = 0,
) -&gt; Tuple[T, ...]:
    &#34;&#34;&#34;Resolve concurrently the input stream and return back in the same order.

    The algorithm makes sure that at any point in time every worker is busy.
    Also, at any point in time there will be at most _number of `workers`_
    tasks being resolved concurrently.

    The `worker_greediness` parameter controlls how much each worker can
    process before waiting for you to retrieve results. This is important when
    the input stream is big as it allows you to control memory usage.

    Usage:
        &gt;&gt;&gt; async def do(n):
                print(f&#39;running: {n}&#39;)
                await sleep(1)
                print(f&#39;returning: {n}&#39;)
                return n

        &gt;&gt;&gt; iterable = map(do, range(5))

        &gt;&gt;&gt; results = await collect(iterable, workers=2)

        &gt;&gt;&gt; print(f&#39;results: {results}&#39;)

    Output:
        ```
        running: 0
        running: 1
        returning: 0
        returning: 1
        running: 2
        running: 3
        returning: 2
        returning: 3
        running: 4
        returning: 4
        results: (0, 1, 2, 3, 4)
        ```

    .. tip::
        This is similar to asyncio.as_completed. However this returns results
        in order and allows you to control how much resources are consumed
        throughout the execution, for instance:

        - How many open files will be opened at the same time
        - How many HTTP requests will be performed to a service (rate limit)
        - How many sockets will be opened concurrently
        - Etc

        This is useful for finite resources, for instance: the number
        of sockets provided by the operative system is limited; going beyond it
        would make the kernel to kill the program abruptly.

    Args:
        awaitables: An iterable (generator, list, tuple, set, etc) of
            awaitables (coroutine, asyncio.Task, or asyncio.Future).
        workers: The number of independent workers that will be processing
            the input stream.
        worker_greediness: How much tasks can a worker process before waiting
            for you to retrieve its results. 0 means unlimited.

    Yields:
        A future with the result of the next ready task. Futures are yielded in
        the same order of the input stream (opposite to asyncio.as_completed)

    .. tip::
        This approach may be many times faster than batching because
        workers are independent of each other and they are constantly fetching
        the next task as soon as they are free (if greediness allows them).

    .. tip::
        If awaitables is an instance of Sized (has `__len__` prototype).
        This function will launch at most `len(awaitables)` workers.
    &#34;&#34;&#34;
    return tuple([
        await elem
        for elem in resolve(
            awaitables,
            workers=workers,
            worker_greediness=worker_greediness,
        )
    ])</code></pre>
</details>
</dd>
<dt id="aioextensions.resolve"><code class="name flex">
<span>def <span class="ident">resolve</span></span>(<span>awaitables: Iterable[Awaitable[~T]], *, workers: int = 1024, worker_greediness: int = 0) ‑> Iterable[Awaitable[~T]]</span>
</code></dt>
<dd>
<div class="desc"><p>Resolve concurrently the input stream and yield back in the same order.</p>
<p>This works very similar to <code><a title="aioextensions.collect" href="#aioextensions.collect">collect()</a></code> except it's lazy and allows you
to handle exceptions on singular elements.</p>
<h2 id="usage">Usage</h2>
<pre><code class="python">&gt;&gt;&gt; async def do(n):
        print(f'running: {n}')
        await asyncio.sleep(1)
        print(f'returning: {n}')
        return n
</code></pre>
<pre><code class="python">&gt;&gt;&gt; iterable = map(do, range(5))
</code></pre>
<pre><code class="python">&gt;&gt;&gt; for next in resolve(iterable, workers=2):
        try:
            print(f'got resolved result: {await next}')
        except:
            pass  # Handle possible exceptions
</code></pre>
<h2 id="output">Output</h2>
<pre><code>running: 0
running: 1
returning: 0
returning: 1
got resolved result: 0
got resolved result: 1
running: 2
running: 3
returning: 2
returning: 3
got resolved result: 2
got resolved result: 3
running: 4
returning: 4
got resolved result: 4
</code></pre>
<p>See <code><a title="aioextensions.collect" href="#aioextensions.collect">collect()</a></code> for more information on the algorithm used and parameters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/src/aioextensions/__init__.py#L490-L580" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def resolve(  # noqa: mccabe
    awaitables: Iterable[Awaitable[T]],
    *,
    workers: int = 1024,
    worker_greediness: int = 0,
) -&gt; Iterable[Awaitable[T]]:
    &#34;&#34;&#34;Resolve concurrently the input stream and yield back in the same order.

    This works very similar to `collect` except it&#39;s lazy and allows you
    to handle exceptions on singular elements.

    Usage:
        &gt;&gt;&gt; async def do(n):
                print(f&#39;running: {n}&#39;)
                await asyncio.sleep(1)
                print(f&#39;returning: {n}&#39;)
                return n

        &gt;&gt;&gt; iterable = map(do, range(5))

        &gt;&gt;&gt; for next in resolve(iterable, workers=2):
                try:
                    print(f&#39;got resolved result: {await next}&#39;)
                except:
                    pass  # Handle possible exceptions

    Output:
        ```
        running: 0
        running: 1
        returning: 0
        returning: 1
        got resolved result: 0
        got resolved result: 1
        running: 2
        running: 3
        returning: 2
        returning: 3
        got resolved result: 2
        got resolved result: 3
        running: 4
        returning: 4
        got resolved result: 4
        ```

    See `collect` for more information on the algorithm used and parameters.
    &#34;&#34;&#34;
    if workers &lt; 1:
        raise ValueError(&#39;workers must be &gt;= 1&#39;)
    if worker_greediness &lt; 0:
        raise ValueError(&#39;worker_greediness must be &gt;= 0&#39;)

    if hasattr(awaitables, &#39;__len__&#39;):
        workers = min(workers, len(awaitables))  # type: ignore

    loop = asyncio.get_event_loop()
    store: Dict[int, asyncio.Queue] = {}
    stream, stream_copy = tee(enumerate(awaitables))
    stream_finished = asyncio.Event()
    workers_up = asyncio.Event()
    workers_tasks: Dict[int, asyncio.Task] = {}

    async def worker() -&gt; None:
        done: asyncio.Queue = asyncio.Queue(worker_greediness)
        for index, awaitable in stream:
            store[index] = done
            future = loop.create_future()
            future.set_result(await schedule(awaitable, loop=loop))
            await done.put(future)
            workers_up.set()
        workers_up.set()
        stream_finished.set()

    async def start_workers() -&gt; None:
        for index in range(workers):
            if stream_finished.is_set():
                break
            workers_tasks[index] = asyncio.create_task(worker())
            await force_loop_cycle()
        await workers_up.wait()

    async def get_one(index: int) -&gt; Awaitable[T]:
        if not workers_tasks:
            await start_workers()

        awaitable = await store.pop(index).get()
        result: Awaitable[T] = (await awaitable).result()
        return result

    for index, _ in stream_copy:
        yield cast(Awaitable[T], get_one(index))</code></pre>
</details>
</dd>
<dt id="aioextensions.force_loop_cycle"><code class="name flex">
<span>async def <span class="ident">force_loop_cycle</span></span>(<span>) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Force the event loop to perform one cycle.</p>
<p>This can be used to suspend the execution of the current coroutine and
yield control back to the event-loop until the next cycle.</p>
<p>Can be seen as a forceful switch of control between threads.
Useful for cooperative initialization.</p>
<h2 id="usage">Usage</h2>
<pre><code class="python">&gt;&gt;&gt; await forceforce_loop_cycle()
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/src/aioextensions/__init__.py#L583-L597" class="git-link">Browse git</a>
</summary>
<pre><code class="python">async def force_loop_cycle() -&gt; None:
    &#34;&#34;&#34;Force the event loop to perform one cycle.

    This can be used to suspend the execution of the current coroutine and
    yield control back to the event-loop until the next cycle.

    Can be seen as a forceful switch of control between threads.
    Useful for cooperative initialization.

    Usage:

        &gt;&gt;&gt; await forceforce_loop_cycle()

    &#34;&#34;&#34;
    await asyncio.sleep(0)</code></pre>
</details>
</dd>
<dt id="aioextensions.generate_in_thread"><code class="name flex">
<span>async def <span class="ident">generate_in_thread</span></span>(<span>generator_func: Callable[..., Generator[~Y, ~S, NoneType]], *args: Any, **kwargs: Any) ‑> AsyncGenerator[~Y, ~S]</span>
</code></dt>
<dd>
<div class="desc"><p>Mimic <code>generator_func(*args, **kwargs)</code> in the configured thread pool.</p>
<p>Note that <code>generator_func(*args, **kwargs)</code> may return a generator or an
interator and both cases are handled rightfully.</p>
<h2 id="usage">Usage</h2>
<pre><code class="python">&gt;&gt;&gt; from os import scandir
</code></pre>
<pre><code class="python">&gt;&gt;&gt; async for entry in generate_in_thread(scandir, '.'):
        print(entry.name)
</code></pre>
<h2 id="output">Output</h2>
<pre><code>.gitignore
LICENSE.md
README.md
...
</code></pre>
<p>Calls to the generator are done serially and not concurrently.</p>
<p>The benefit of wrapping a generator with this function is that the
event-loop is free to schedule and wait another tasks in the mean time.
For instance, in a web server.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/src/aioextensions/__init__.py#L600-L643" class="git-link">Browse git</a>
</summary>
<pre><code class="python">async def generate_in_thread(
    generator_func: Callable[..., Generator[Y, S, None]],
    *args: Any,
    **kwargs: Any,
) -&gt; AsyncGenerator[Y, S]:
    &#34;&#34;&#34;Mimic `generator_func(*args, **kwargs)` in the configured thread pool.

    Note that `generator_func(*args, **kwargs)` may return a generator or an
    interator and both cases are handled rightfully.

    Usage:

        &gt;&gt;&gt; from os import scandir

        &gt;&gt;&gt; async for entry in generate_in_thread(scandir, &#39;.&#39;):
                print(entry.name)

    Output:
        ```
        .gitignore
        LICENSE.md
        README.md
        ...
        ```

    Calls to the generator are done serially and not concurrently.

    The benefit of wrapping a generator with this function is that the
    event-loop is free to schedule and wait another tasks in the mean time.
    For instance, in a web server.
    &#34;&#34;&#34;
    gen: Generator[Y, S, None] = generator_func(*args, **kwargs)
    gen_sent: Any = None

    def gen_next(val: S) -&gt; Y:
        with suppress(StopIteration):
            return gen.send(val) if hasattr(gen, &#39;send&#39;) else next(gen)
        raise StopAsyncIteration()

    while True:
        try:
            gen_sent = yield await in_thread(gen_next, gen_sent)
        except StopAsyncIteration:
            return</code></pre>
</details>
</dd>
<dt id="aioextensions.schedule"><code class="name flex">
<span>def <span class="ident">schedule</span></span>(<span>awaitable: Awaitable[~T], *, loop: Union[asyncio.events.AbstractEventLoop, NoneType] = None) ‑> 'Awaitable[asyncio.Future[T]]'</span>
</code></dt>
<dd>
<div class="desc"><p>Schedule an awaitable in the event loop and return a wrapper for it.</p>
<h2 id="usage">Usage</h2>
<pre><code class="python">&gt;&gt;&gt; async def do(n):
        print(f'running: {n}')
        await sleep(1)
        print(f'returning: {n}')
</code></pre>
<pre><code class="python">&gt;&gt;&gt; task = schedule(do(3))  # Task is executing in the background now
</code></pre>
<pre><code class="python">&gt;&gt;&gt; print('other work is being done here')
</code></pre>
<pre><code class="python">&gt;&gt;&gt; task_result = await task  # Wait until the task is ready
</code></pre>
<pre><code class="python">&gt;&gt;&gt; print(f'result: {task_result.result()}')  # may rise if do() raised
</code></pre>
<h2 id="output">Output</h2>
<pre><code>other work is being done here
doing: 3
returning: 3
3
</code></pre>
<p>This works very similar to asyncio.create_task. The main difference is that
the result (or exception) can be accessed via exception() or result()
methods.</p>
<p>If an exception was raised by the awaitable, it will be propagated only at
the moment result() is called and never otherwise.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/src/aioextensions/__init__.py#L646-L692" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def schedule(
    awaitable: Awaitable[T],
    *,
    loop: Optional[asyncio.AbstractEventLoop] = None,
) -&gt; &#39;Awaitable[asyncio.Future[T]]&#39;:
    &#34;&#34;&#34;Schedule an awaitable in the event loop and return a wrapper for it.

    Usage:

        &gt;&gt;&gt; async def do(n):
                print(f&#39;running: {n}&#39;)
                await sleep(1)
                print(f&#39;returning: {n}&#39;)

        &gt;&gt;&gt; task = schedule(do(3))  # Task is executing in the background now

        &gt;&gt;&gt; print(&#39;other work is being done here&#39;)

        &gt;&gt;&gt; task_result = await task  # Wait until the task is ready

        &gt;&gt;&gt; print(f&#39;result: {task_result.result()}&#39;)  # may rise if do() raised

    Output:

        ```
        other work is being done here
        doing: 3
        returning: 3
        3
        ```

    This works very similar to asyncio.create_task. The main difference is that
    the result (or exception) can be accessed via exception() or result()
    methods.

    If an exception was raised by the awaitable, it will be propagated only at
    the moment result() is called and never otherwise.
    &#34;&#34;&#34;
    wrapper = (loop or asyncio.get_event_loop()).create_future()

    def _done_callback(future: asyncio.Future) -&gt; None:
        if not wrapper.done():
            wrapper.set_result(future)

    asyncio.create_task(awaitable).add_done_callback(_done_callback)

    return wrapper</code></pre>
</details>
</dd>
<dt id="aioextensions.run_decorator"><code class="name flex">
<span>def <span class="ident">run_decorator</span></span>(<span>function: ~F) ‑> ~F</span>
</code></dt>
<dd>
<div class="desc"><p>Decorator to turn an asynchronous function into a synchronous one.</p>
<h2 id="example">Example</h2>
<pre><code class="python">&gt;&gt;&gt; @run_decorator
    async def do(a, b=0):
        return a + b
</code></pre>
<pre><code class="python">&gt;&gt;&gt; do(1, b=2) == 3
</code></pre>
<p>This can be used as a bridge between synchronous and asynchronous code.
We use it mostly in tests for its convenience over pytest-asyncio plugin.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/src/aioextensions/__init__.py#L753-L771" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def run_decorator(function: F) -&gt; F:
    &#34;&#34;&#34;Decorator to turn an asynchronous function into a synchronous one.

    Example:
        &gt;&gt;&gt; @run_decorator
            async def do(a, b=0):
                return a + b

        &gt;&gt;&gt; do(1, b=2) == 3

    This can be used as a bridge between synchronous and asynchronous code.
    We use it mostly in tests for its convenience over pytest-asyncio plugin.
    &#34;&#34;&#34;

    @wraps(function)
    def wrapper(*args: Any, **kwargs: Any) -&gt; Any:
        return run(function(*args, **kwargs))

    return cast(F, wrapper)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="aioextensions.ExecutorPool"><code class="flex name class">
<span>class <span class="ident">ExecutorPool</span></span>
<span>(</span><span>cls: Union[Type[concurrent.futures.process.ProcessPoolExecutor], Type[concurrent.futures.thread.ThreadPoolExecutor]])</span>
</code></dt>
<dd>
<div class="desc"><p>Object representing a pool of Processes or Threads.</p>
<p>The actual pool is created at <code>initialization</code> time
and it is empty until that.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/src/aioextensions/__init__.py#L705-L750" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ExecutorPool:
    &#34;&#34;&#34;Object representing a pool of Processes or Threads.

    The actual pool is created at `initialization` time
    and it is empty until that.
    &#34;&#34;&#34;
    def __init__(
        self,
        cls: Union[
            Type[ProcessPoolExecutor],
            Type[ThreadPoolExecutor],
        ],
    ) -&gt; None:
        self._cls = cls
        self._pool: Optional[Executor] = None

    def initialize(self, *, max_workers: Optional[int] = None) -&gt; None:
        &#34;&#34;&#34;Initialize the executor with a cap of at most `max_workers`.

        Workers are created on-demand as needed or never created at all
        if never needed.
        &#34;&#34;&#34;
        if self._pool is not None:
            self._pool.shutdown(wait=False)

        self._pool = self._cls(max_workers=max_workers)

    def shutdown(self, *, wait: bool) -&gt; None:
        &#34;&#34;&#34;Shut down the executor and (optionally) waits for workers to finish.
        &#34;&#34;&#34;
        if self._pool is not None:
            self._pool.shutdown(wait=wait)
            self._pool = None

    @property
    def pool(self) -&gt; Executor:
        &#34;&#34;&#34;Low level pool of workers held by the executor, may be None.&#34;&#34;&#34;
        if self._pool is None:
            raise RuntimeError(&#39;Must call initialize first&#39;)

        return self._pool

    @property
    def initialized(self) -&gt; bool:
        &#34;&#34;&#34;Return true if the executor is initialized and ready to process.&#34;&#34;&#34;
        return self._pool is not None</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="aioextensions.ExecutorPool.pool"><code class="name">var <span class="ident">pool</span> : concurrent.futures._base.Executor</code></dt>
<dd>
<div class="desc"><p>Low level pool of workers held by the executor, may be None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/src/aioextensions/__init__.py#L739-L745" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@property
def pool(self) -&gt; Executor:
    &#34;&#34;&#34;Low level pool of workers held by the executor, may be None.&#34;&#34;&#34;
    if self._pool is None:
        raise RuntimeError(&#39;Must call initialize first&#39;)

    return self._pool</code></pre>
</details>
</dd>
<dt id="aioextensions.ExecutorPool.initialized"><code class="name">var <span class="ident">initialized</span> : bool</code></dt>
<dd>
<div class="desc"><p>Return true if the executor is initialized and ready to process.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/src/aioextensions/__init__.py#L747-L750" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@property
def initialized(self) -&gt; bool:
    &#34;&#34;&#34;Return true if the executor is initialized and ready to process.&#34;&#34;&#34;
    return self._pool is not None</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="aioextensions.ExecutorPool.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, *, max_workers: Union[int, NoneType] = None) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the executor with a cap of at most <code>max_workers</code>.</p>
<p>Workers are created on-demand as needed or never created at all
if never needed.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/src/aioextensions/__init__.py#L721-L730" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def initialize(self, *, max_workers: Optional[int] = None) -&gt; None:
    &#34;&#34;&#34;Initialize the executor with a cap of at most `max_workers`.

    Workers are created on-demand as needed or never created at all
    if never needed.
    &#34;&#34;&#34;
    if self._pool is not None:
        self._pool.shutdown(wait=False)

    self._pool = self._cls(max_workers=max_workers)</code></pre>
</details>
</dd>
<dt id="aioextensions.ExecutorPool.shutdown"><code class="name flex">
<span>def <span class="ident">shutdown</span></span>(<span>self, *, wait: bool) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Shut down the executor and (optionally) waits for workers to finish.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/kamadorueda/aioextensions/blob/latest/src/aioextensions/__init__.py#L732-L737" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def shutdown(self, *, wait: bool) -&gt; None:
    &#34;&#34;&#34;Shut down the executor and (optionally) waits for workers to finish.
    &#34;&#34;&#34;
    if self._pool is not None:
        self._pool.shutdown(wait=wait)
        self._pool = None</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#rationale">Rationale</a><ul>
<li><a href="#cpu-bound-tasks">CPU bound tasks</a></li>
<li><a href="#io-bound-tasks">IO bound tasks</a></li>
</ul>
</li>
<li><a href="#speed-and-costs-matter">Speed and costs matter</a></li>
<li><a href="#concurrency-model">Concurrency model</a><ul>
<li><a href="#solving-cpu-bound-tasks-efficiently">Solving CPU bound tasks efficiently</a></li>
<li><a href="#solving-io-bound-tasks-efficiently">Solving IO bound tasks efficiently</a></li>
</ul>
</li>
<li><a href="#installing">Installing</a></li>
<li><a href="#using">Using</a></li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="aioextensions.CPU_CORES" href="#aioextensions.CPU_CORES">CPU_CORES</a></code></li>
<li><code><a title="aioextensions.PROCESS_POOL" href="#aioextensions.PROCESS_POOL">PROCESS_POOL</a></code></li>
<li><code><a title="aioextensions.THREAD_POOL" href="#aioextensions.THREAD_POOL">THREAD_POOL</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="aioextensions.run" href="#aioextensions.run">run</a></code></li>
<li><code><a title="aioextensions.in_thread" href="#aioextensions.in_thread">in_thread</a></code></li>
<li><code><a title="aioextensions.in_process" href="#aioextensions.in_process">in_process</a></code></li>
<li><code><a title="aioextensions.rate_limited" href="#aioextensions.rate_limited">rate_limited</a></code></li>
<li><code><a title="aioextensions.collect" href="#aioextensions.collect">collect</a></code></li>
<li><code><a title="aioextensions.resolve" href="#aioextensions.resolve">resolve</a></code></li>
<li><code><a title="aioextensions.force_loop_cycle" href="#aioextensions.force_loop_cycle">force_loop_cycle</a></code></li>
<li><code><a title="aioextensions.generate_in_thread" href="#aioextensions.generate_in_thread">generate_in_thread</a></code></li>
<li><code><a title="aioextensions.schedule" href="#aioextensions.schedule">schedule</a></code></li>
<li><code><a title="aioextensions.run_decorator" href="#aioextensions.run_decorator">run_decorator</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="aioextensions.ExecutorPool" href="#aioextensions.ExecutorPool">ExecutorPool</a></code></h4>
<ul class="">
<li><code><a title="aioextensions.ExecutorPool.initialize" href="#aioextensions.ExecutorPool.initialize">initialize</a></code></li>
<li><code><a title="aioextensions.ExecutorPool.shutdown" href="#aioextensions.ExecutorPool.shutdown">shutdown</a></code></li>
<li><code><a title="aioextensions.ExecutorPool.pool" href="#aioextensions.ExecutorPool.pool">pool</a></code></li>
<li><code><a title="aioextensions.ExecutorPool.initialized" href="#aioextensions.ExecutorPool.initialized">initialized</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.4</a>.</p>
</footer>
</body>
</html>